{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71117488-555d-414e-b82b-8024c59230fd",
   "metadata": {},
   "source": [
    "# Amazon Redshift Data Warehouse Workshop\n",
    "\n",
    "This hands-on workshop guides you through creating, configuring, and using an Amazon Redshift cluster for data warehousing. Each cell contains clear instructions that can be executed sequentially.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with appropriate permissions\n",
    "- AWS CLI configured with access credentials\n",
    "- Python 3.8+ with the following packages installed:\n",
    "  - boto3\n",
    "  - pandas\n",
    "  - psycopg2 or psycopg2-binary\n",
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34364aa3-0c55-4712-a830-465eb8174e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjij/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa8d73-3a31-4c13-9e20-002ff252c72a",
   "metadata": {},
   "source": [
    "### 1.2 Set Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192c4148-146d-4aae-8397-573475282c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "region = 'ap-south-1'  # Change to your preferred region\n",
    "\n",
    "# Redshift Cluster Configuration\n",
    "cluster_identifier = 'redshift-workshop'\n",
    "node_type = 'dc2.large'  # For workshop, using smaller node type\n",
    "number_of_nodes = 2\n",
    "db_name = 'dwh'\n",
    "master_username = 'admin'\n",
    "master_password = 'Redshift123!'  # Use a secure password in production\n",
    "\n",
    "# VPC Configuration - will use default VPC for simplicity\n",
    "vpc_id = 'default'  # Will be populated later\n",
    "\n",
    "# S3 Configuration\n",
    "s3_bucket = f'redshift-workshop-{int(time.time())}'  # Unique bucket name\n",
    "sample_data_prefix = 'sample-data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953718c7-2163-4486-bf61-9d35541e3647",
   "metadata": {},
   "source": [
    "## 2. Create Resources\n",
    "\n",
    "### 2.1 Create IAM Role for Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275a9ad4-18f6-438a-b004-8d34d900f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created IAM role: RedshiftWorkshopRole with ARN: arn:aws:iam::359373501475:role/RedshiftWorkshopRole\n",
      "Attached S3 read policy to role\n"
     ]
    }
   ],
   "source": [
    "def create_redshift_service_role():\n",
    "    \"\"\"Create IAM role for Redshift to access S3\"\"\"\n",
    "    iam = boto3.client('iam', region_name=region)\n",
    "    \n",
    "    # Define trust policy\n",
    "    trust_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"redshift.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create the role\n",
    "        response = iam.create_role(\n",
    "            RoleName='RedshiftWorkshopRole',\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "            Description='Role for Redshift workshop to access S3'\n",
    "        )\n",
    "        \n",
    "        role_arn = response['Role']['Arn']\n",
    "        print(f\"Created IAM role: RedshiftWorkshopRole with ARN: {role_arn}\")\n",
    "        \n",
    "        # Attach S3 read policy\n",
    "        iam.attach_role_policy(\n",
    "            RoleName='RedshiftWorkshopRole',\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    "        )\n",
    "        \n",
    "        print(\"Attached S3 read policy to role\")\n",
    "        return role_arn\n",
    "    \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "            print(\"IAM role RedshiftWorkshopRole already exists\")\n",
    "            response = iam.get_role(RoleName='RedshiftWorkshopRole')\n",
    "            return response['Role']['Arn']\n",
    "        else:\n",
    "            print(f\"Error creating IAM role: {e}\")\n",
    "            raise\n",
    "\n",
    "# Create the IAM role\n",
    "redshift_role_arn = create_redshift_service_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e74ee-bea3-4a86-9869-0524870c509c",
   "metadata": {},
   "source": [
    "### 2.2 Create S3 Bucket and Upload Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ecf05fe-0668-4dcb-9a13-5f08e1c62405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created S3 bucket: redshift-workshop-1743684928\n",
      "Uploaded sample data to s3://redshift-workshop-1743684928/sample-data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_s3_bucket_and_sample_data():\n",
    "    \"\"\"Create S3 bucket and generate sample data\"\"\"\n",
    "    s3 = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region == 'us-east-1':\n",
    "            s3.create_bucket(Bucket=s3_bucket)\n",
    "        else:\n",
    "            s3.create_bucket(\n",
    "                Bucket=s3_bucket,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"Created S3 bucket: {s3_bucket}\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error creating bucket: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Generate sample customers data\n",
    "    customers_data = pd.DataFrame({\n",
    "        'customer_id': range(1, 101),\n",
    "        'customer_name': [f'Customer {i}' for i in range(1, 101)],\n",
    "        'email': [f'customer{i}@example.com' for i in range(1, 101)],\n",
    "        'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'] * 20,\n",
    "        'state': ['NY', 'CA', 'IL', 'TX', 'AZ'] * 20,\n",
    "        'country': ['USA'] * 100,\n",
    "        'registration_date': pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "    })\n",
    "    \n",
    "    # Generate sample products data\n",
    "    products_data = pd.DataFrame({\n",
    "        'product_id': range(1, 51),\n",
    "        'product_name': [f'Product {i}' for i in range(1, 51)],\n",
    "        'category': ['Electronics', 'Clothing', 'Home', 'Books', 'Toys'] * 10,\n",
    "        'price': [round(i * 9.99, 2) for i in range(1, 51)],\n",
    "        'cost': [round(i * 5.99, 2) for i in range(1, 51)]\n",
    "    })\n",
    "    \n",
    "    # Generate sample sales data\n",
    "    import numpy as np\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    sales_data = []\n",
    "    for i in range(1, 1001):\n",
    "        customer_id = np.random.randint(1, 101)\n",
    "        product_id = np.random.randint(1, 51)\n",
    "        product_price = products_data.loc[product_id-1, 'price']\n",
    "        quantity = np.random.randint(1, 5)\n",
    "        \n",
    "        sales_data.append({\n",
    "            'sale_id': i,\n",
    "            'customer_id': customer_id,\n",
    "            'product_id': product_id,\n",
    "            'sale_date': pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365)),\n",
    "            'quantity': quantity,\n",
    "            'unit_price': product_price,\n",
    "            'total_amount': quantity * product_price\n",
    "        })\n",
    "    \n",
    "    sales_df = pd.DataFrame(sales_data)\n",
    "    \n",
    "    # Save to local files first\n",
    "    customers_data.to_csv('customers.csv', index=False)\n",
    "    products_data.to_csv('products.csv', index=False)\n",
    "    sales_df.to_csv('sales.csv', index=False)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(s3_bucket, f\"{sample_data_prefix}customers.csv\").upload_file('customers.csv')\n",
    "    s3_resource.Object(s3_bucket, f\"{sample_data_prefix}products.csv\").upload_file('products.csv')\n",
    "    s3_resource.Object(s3_bucket, f\"{sample_data_prefix}sales.csv\").upload_file('sales.csv')\n",
    "    \n",
    "    print(f\"Uploaded sample data to s3://{s3_bucket}/{sample_data_prefix}\")\n",
    "    \n",
    "    # Clean up local files\n",
    "    os.remove('customers.csv')\n",
    "    os.remove('products.csv')\n",
    "    os.remove('sales.csv')\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Create bucket and upload sample data\n",
    "create_s3_bucket_and_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acc516-dde5-47a9-b200-43adb90ef09d",
   "metadata": {},
   "source": [
    "### 2.3 Create Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f45c8d-bc29-4a74-84d3-176f336a86ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default VPC: vpc-6f769c04\n",
      "Created security group: sg-0f003daafe19fedfa\n",
      "Creating Redshift cluster: redshift-workshop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_redshift_cluster():\n",
    "    \"\"\"Create a Redshift cluster\"\"\"\n",
    "    redshift = boto3.client('redshift', region_name=region)\n",
    "    \n",
    "    # Get default VPC ID\n",
    "    ec2 = boto3.client('ec2', region_name=region)\n",
    "    vpcs = ec2.describe_vpcs(\n",
    "        Filters=[{'Name': 'isDefault', 'Values': ['true']}]\n",
    "    )\n",
    "    \n",
    "    if not vpcs['Vpcs']:\n",
    "        raise Exception(\"No default VPC found in this account/region\")\n",
    "    \n",
    "    global vpc_id\n",
    "    vpc_id = vpcs['Vpcs'][0]['VpcId']\n",
    "    print(f\"Using default VPC: {vpc_id}\")\n",
    "    \n",
    "    # Create security group for Redshift\n",
    "    try:\n",
    "        sg_response = ec2.create_security_group(\n",
    "            GroupName='redshift-workshop-sg',\n",
    "            Description='Security group for Redshift workshop',\n",
    "            VpcId=vpc_id\n",
    "        )\n",
    "        security_group_id = sg_response['GroupId']\n",
    "        \n",
    "        # Add ingress rule to allow connections\n",
    "        ec2.authorize_security_group_ingress(\n",
    "            GroupId=security_group_id,\n",
    "            IpPermissions=[\n",
    "                {\n",
    "                    'IpProtocol': 'tcp',\n",
    "                    'FromPort': 5439,\n",
    "                    'ToPort': 5439,\n",
    "                    'IpRanges': [{'CidrIp': '0.0.0.0/0'}]  # In production, restrict this!\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"Created security group: {security_group_id}\")\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'InvalidGroup.Duplicate':\n",
    "            # Get existing security group\n",
    "            sgs = ec2.describe_security_groups(\n",
    "                Filters=[\n",
    "                    {'Name': 'group-name', 'Values': ['redshift-workshop-sg']},\n",
    "                    {'Name': 'vpc-id', 'Values': [vpc_id]}\n",
    "                ]\n",
    "            )\n",
    "            security_group_id = sgs['SecurityGroups'][0]['GroupId']\n",
    "            print(f\"Using existing security group: {security_group_id}\")\n",
    "        else:\n",
    "            print(f\"Error creating security group: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Create Redshift cluster\n",
    "    try:\n",
    "        response = redshift.create_cluster(\n",
    "            ClusterIdentifier=cluster_identifier,\n",
    "            NodeType=node_type,\n",
    "            NumberOfNodes=number_of_nodes,\n",
    "            DBName=db_name,\n",
    "            MasterUsername=master_username,\n",
    "            MasterUserPassword=master_password,\n",
    "            VpcSecurityGroupIds=[security_group_id],\n",
    "            PubliclyAccessible=True,  # For workshop purposes\n",
    "            IamRoles=[redshift_role_arn]\n",
    "        )\n",
    "        \n",
    "        print(f\"Creating Redshift cluster: {cluster_identifier}\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ClusterAlreadyExists':\n",
    "            print(f\"Redshift cluster {cluster_identifier} already exists\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error creating Redshift cluster: {e}\")\n",
    "            raise\n",
    "\n",
    "# Create the Redshift cluster\n",
    "create_redshift_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2744648-f39c-4563-838b-0690c50d2c2f",
   "metadata": {},
   "source": [
    "### 2.4 Wait for Cluster to be Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b686cc-bd28-42ad-a16d-6a440a3a8bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for cluster redshift-workshop to be available...\n",
      "Cluster status: creating. Waiting...\n",
      "Cluster status: creating. Waiting...\n",
      "Cluster status: creating. Waiting...\n",
      "Cluster status: creating. Waiting...\n",
      "Cluster redshift-workshop is now available\n",
      "Cluster endpoint: redshift-workshop.c1e1ajkaqemc.ap-south-1.redshift.amazonaws.com\n",
      "Cluster port: 5439\n"
     ]
    }
   ],
   "source": [
    "def wait_for_cluster_available():\n",
    "    \"\"\"Wait for the Redshift cluster to be in 'available' state\"\"\"\n",
    "    redshift = boto3.client('redshift', region_name=region)\n",
    "    \n",
    "    print(f\"Waiting for cluster {cluster_identifier} to be available...\")\n",
    "    \n",
    "    while True:\n",
    "        response = redshift.describe_clusters(ClusterIdentifier=cluster_identifier)\n",
    "        status = response['Clusters'][0]['ClusterStatus']\n",
    "        \n",
    "        if status == 'available':\n",
    "            print(f\"Cluster {cluster_identifier} is now available\")\n",
    "            endpoint = response['Clusters'][0]['Endpoint']['Address']\n",
    "            port = response['Clusters'][0]['Endpoint']['Port']\n",
    "            return endpoint, port\n",
    "        \n",
    "        print(f\"Cluster status: {status}. Waiting...\")\n",
    "        time.sleep(30)  # Check every 30 seconds for workshop purposes\n",
    "\n",
    "# Wait for cluster to be available\n",
    "cluster_endpoint, cluster_port = wait_for_cluster_available()\n",
    "print(f\"Cluster endpoint: {cluster_endpoint}\")\n",
    "print(f\"Cluster port: {cluster_port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f68ba-aa34-455f-9394-a07f60c7161e",
   "metadata": {},
   "source": [
    "## 3. Database Setup and Data Loading\n",
    "\n",
    "### 3.1 Create a Function to Execute SQL Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ca951f-875f-40c5-8ee6-8acb49bbeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(sql, fetch=False):\n",
    "    \"\"\"Execute SQL commands on the Redshift cluster\"\"\"\n",
    "    conn = None\n",
    "    result = None\n",
    "    \n",
    "    try:\n",
    "        # Connect to the Redshift cluster\n",
    "        conn = psycopg2.connect(\n",
    "            host=cluster_endpoint,\n",
    "            port=cluster_port,\n",
    "            dbname=db_name,\n",
    "            user=master_username,\n",
    "            password=master_password\n",
    "        )\n",
    "        \n",
    "        # Set isolation level to autocommit\n",
    "        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "        \n",
    "        # Create a cursor\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Execute the SQL command\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        # Fetch results if requested\n",
    "        if fetch:\n",
    "            result = cur.fetchall()\n",
    "        \n",
    "        # Close cursor\n",
    "        cur.close()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        # Close connection\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2fd06-7f1e-4832-818a-aa8650a13afd",
   "metadata": {},
   "source": [
    "### 3.2 Create Schema and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cc84f3a-6974-4698-93af-b5d129fe0018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created schema and tables\n"
     ]
    }
   ],
   "source": [
    " #First, create the schema\n",
    "create_schema_sql = \"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS analytics;\n",
    "\"\"\"\n",
    "execute_sql(create_schema_sql)\n",
    "\n",
    "# Then create the tables\n",
    "create_tables_sql = \"\"\"\n",
    "-- Set search path\n",
    "SET search_path TO analytics, public;\n",
    "\n",
    "-- Create customers dimension table with ALL distribution\n",
    "CREATE TABLE IF NOT EXISTS analytics.customers (\n",
    "    customer_id INTEGER PRIMARY KEY,\n",
    "    customer_name VARCHAR(100),\n",
    "    email VARCHAR(100),\n",
    "    city VARCHAR(50),\n",
    "    state VARCHAR(50),\n",
    "    country VARCHAR(50),\n",
    "    registration_date DATE\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "SORTKEY(customer_id);\n",
    "\n",
    "-- Create products dimension table with ALL distribution\n",
    "CREATE TABLE IF NOT EXISTS analytics.products (\n",
    "    product_id INTEGER PRIMARY KEY,\n",
    "    product_name VARCHAR(100) NOT NULL,\n",
    "    category VARCHAR(50) NOT NULL,\n",
    "    price DECIMAL(10,2) NOT NULL,\n",
    "    cost DECIMAL(10,2)\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "SORTKEY(product_id);\n",
    "\n",
    "-- Create sales fact table with KEY distribution\n",
    "CREATE TABLE IF NOT EXISTS analytics.sales (\n",
    "    sale_id INTEGER PRIMARY KEY,\n",
    "    customer_id INTEGER NOT NULL REFERENCES analytics.customers(customer_id),\n",
    "    product_id INTEGER NOT NULL REFERENCES analytics.products(product_id),\n",
    "    sale_date DATE NOT NULL,\n",
    "    quantity INTEGER NOT NULL,\n",
    "    unit_price DECIMAL(10,2) NOT NULL,\n",
    "    total_amount DECIMAL(10,2) NOT NULL\n",
    ")\n",
    "DISTKEY(customer_id)\n",
    "COMPOUND SORTKEY(sale_date, customer_id);\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(create_tables_sql)\n",
    "print(\"Created schema and tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8918e8f2-a0d9-4d53-af9c-607194efc702",
   "metadata": {},
   "source": [
    "### 3.3 Load Data from S3 Using COPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a684bece-592e-40cb-a832-67b881c34308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from S3 to Redshift tables\n"
     ]
    }
   ],
   "source": [
    "# Load data into tables using COPY command\n",
    "load_data_sql = f\"\"\"\n",
    "-- Load customers data\n",
    "COPY analytics.customers\n",
    "FROM 's3://{s3_bucket}/{sample_data_prefix}customers.csv'\n",
    "IAM_ROLE '{redshift_role_arn}'\n",
    "FORMAT AS CSV DELIMITER ',' IGNOREHEADER 1\n",
    "REGION '{region}';\n",
    "\n",
    "-- Load products data\n",
    "COPY analytics.products\n",
    "FROM 's3://{s3_bucket}/{sample_data_prefix}products.csv'\n",
    "IAM_ROLE '{redshift_role_arn}'\n",
    "FORMAT AS CSV DELIMITER ',' IGNOREHEADER 1\n",
    "REGION '{region}';\n",
    "\n",
    "-- Load sales data\n",
    "COPY analytics.sales\n",
    "FROM 's3://{s3_bucket}/{sample_data_prefix}sales.csv'\n",
    "IAM_ROLE '{redshift_role_arn}'\n",
    "FORMAT AS CSV DELIMITER ',' IGNOREHEADER 1\n",
    "REGION '{region}';\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(load_data_sql)\n",
    "print(\"Loaded data from S3 to Redshift tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ae2b07-66c0-4d3b-8ea0-887713df957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.4 Verify Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6179f123-7a45-40c6-a14b-5e9bbf468f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table row counts:\n",
      "  products: 50 rows\n",
      "  sales: 1000 rows\n",
      "  customers: 100 rows\n"
     ]
    }
   ],
   "source": [
    "# Verify data was loaded correctly\n",
    "verify_load_sql = \"\"\"\n",
    "SELECT 'customers' AS table_name, COUNT(*) AS row_count FROM analytics.customers\n",
    "UNION ALL\n",
    "SELECT 'products', COUNT(*) FROM analytics.products\n",
    "UNION ALL\n",
    "SELECT 'sales', COUNT(*) FROM analytics.sales;\n",
    "\"\"\"\n",
    "\n",
    "row_counts = execute_sql(verify_load_sql, fetch=True)\n",
    "print(\"Table row counts:\")\n",
    "for row in row_counts:\n",
    "    print(f\"  {row[0]}: {row[1]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755f92d-95dc-467f-b1b2-f9112de947a2",
   "metadata": {},
   "source": [
    "## 4. Running Analytical Queries\n",
    "\n",
    "### 4.1 Create a Materialized View for Common Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63a3a327-ebe7-4598-8fb6-de75279f62fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created materialized view for sales by category\n"
     ]
    }
   ],
   "source": [
    "# Create a materialized view for sales by category\n",
    "materialized_view_sql = \"\"\"\n",
    "-- Create materialized view for daily sales by category\n",
    "CREATE MATERIALIZED VIEW analytics.daily_sales_by_category AS\n",
    "SELECT \n",
    "    s.sale_date,\n",
    "    p.category,\n",
    "    COUNT(DISTINCT s.customer_id) AS unique_customers,\n",
    "    SUM(s.quantity) AS units_sold,\n",
    "    SUM(s.total_amount) AS total_revenue,\n",
    "    SUM(s.total_amount - (s.quantity * p.cost)) AS gross_profit\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY s.sale_date, p.category;\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(materialized_view_sql)\n",
    "print(\"Created materialized view for sales by category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d70e62-6d71-4cd8-b8a3-890354367354",
   "metadata": {},
   "source": [
    "### 4.2 Run Analytical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3f744dc-7e16-451e-af4d-625f96f7455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 selling products by revenue:\n",
      "  Product ID: 49, Name: Product 49, Category: Books, Quantity: 63, Revenue: $30839.13\n",
      "  Product ID: 50, Name: Product 50, Category: Toys, Quantity: 56, Revenue: $27972.00\n",
      "  Product ID: 47, Name: Product 47, Category: Clothing, Quantity: 56, Revenue: $26293.68\n",
      "  Product ID: 33, Name: Product 33, Category: Home, Quantity: 73, Revenue: $24065.91\n",
      "  Product ID: 40, Name: Product 40, Category: Toys, Quantity: 60, Revenue: $23976.00\n",
      "  Product ID: 46, Name: Product 46, Category: Electronics, Quantity: 52, Revenue: $23896.08\n",
      "  Product ID: 48, Name: Product 48, Category: Home, Quantity: 49, Revenue: $23496.48\n",
      "  Product ID: 42, Name: Product 42, Category: Clothing, Quantity: 55, Revenue: $23076.90\n",
      "  Product ID: 34, Name: Product 34, Category: Books, Quantity: 67, Revenue: $22757.22\n",
      "  Product ID: 35, Name: Product 35, Category: Toys, Quantity: 63, Revenue: $22027.91\n",
      "\n",
      "Monthly sales trends:\n",
      "  Month: 2023-01-01 00:00:00, Revenue: $66473.42, Unique Customers: 68\n",
      "  Month: 2023-02-01 00:00:00, Revenue: $44305.64, Unique Customers: 49\n",
      "  Month: 2023-03-01 00:00:00, Revenue: $51808.12, Unique Customers: 57\n",
      "  Month: 2023-04-01 00:00:00, Revenue: $46373.56, Unique Customers: 58\n",
      "  Month: 2023-05-01 00:00:00, Revenue: $51658.28, Unique Customers: 60\n",
      "  Month: 2023-06-01 00:00:00, Revenue: $51218.69, Unique Customers: 58\n",
      "  Month: 2023-07-01 00:00:00, Revenue: $42657.28, Unique Customers: 51\n",
      "  Month: 2023-08-01 00:00:00, Revenue: $70988.92, Unique Customers: 57\n",
      "  Month: 2023-09-01 00:00:00, Revenue: $52307.63, Unique Customers: 51\n",
      "  Month: 2023-10-01 00:00:00, Revenue: $52237.68, Unique Customers: 61\n",
      "  Month: 2023-11-01 00:00:00, Revenue: $61378.49, Unique Customers: 54\n",
      "  Month: 2023-12-01 00:00:00, Revenue: $61128.78, Unique Customers: 57\n",
      "\n",
      "Top customer segments by spend:\n",
      "  Location: Houston, TX, Category: Toys, Customers: 19, Total Spent: $36563.36\n",
      "  Location: Phoenix, AZ, Category: Books, Customers: 19, Total Spent: $34395.56\n",
      "  Location: New York, NY, Category: Books, Customers: 17, Total Spent: $34165.79\n",
      "  Location: Phoenix, AZ, Category: Home, Customers: 16, Total Spent: $33696.27\n",
      "  Location: Chicago, IL, Category: Toys, Customers: 19, Total Spent: $33666.27\n",
      "  Location: Los Angeles, CA, Category: Clothing, Customers: 18, Total Spent: $33056.89\n",
      "  Location: Phoenix, AZ, Category: Toys, Customers: 19, Total Spent: $31967.96\n",
      "  Location: New York, NY, Category: Toys, Customers: 19, Total Spent: $29570.36\n",
      "  Location: Houston, TX, Category: Electronics, Customers: 18, Total Spent: $28571.40\n",
      "  Location: Chicago, IL, Category: Clothing, Customers: 18, Total Spent: $28051.92\n",
      "  Location: Houston, TX, Category: Home, Customers: 18, Total Spent: $26493.48\n",
      "  Location: Los Angeles, CA, Category: Books, Customers: 16, Total Spent: $26213.74\n",
      "  Location: Chicago, IL, Category: Electronics, Customers: 19, Total Spent: $25914.02\n",
      "  Location: Chicago, IL, Category: Books, Customers: 18, Total Spent: $24625.35\n",
      "  Location: Chicago, IL, Category: Home, Customers: 16, Total Spent: $24375.60\n"
     ]
    }
   ],
   "source": [
    "# Run sample analytical queries\n",
    "\n",
    "# Query 1: Top selling products\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    p.product_id,\n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    SUM(s.quantity) AS total_quantity_sold,\n",
    "    SUM(s.total_amount) AS total_revenue\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY p.product_id, p.product_name, p.category\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display results\n",
    "results1 = execute_sql(query1, fetch=True)\n",
    "print(\"Top 10 selling products by revenue:\")\n",
    "for row in results1:\n",
    "    print(f\"  Product ID: {row[0]}, Name: {row[1]}, Category: {row[2]}, Quantity: {row[3]}, Revenue: ${row[4]:.2f}\")\n",
    "\n",
    "# Query 2: Monthly sales trends\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    DATE_TRUNC('month', sale_date) AS month,\n",
    "    SUM(total_amount) AS monthly_revenue,\n",
    "    COUNT(DISTINCT customer_id) AS unique_customers\n",
    "FROM analytics.sales\n",
    "GROUP BY DATE_TRUNC('month', sale_date)\n",
    "ORDER BY month;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display results\n",
    "results2 = execute_sql(query2, fetch=True)\n",
    "print(\"\\nMonthly sales trends:\")\n",
    "for row in results2:\n",
    "    print(f\"  Month: {row[0]}, Revenue: ${row[1]:.2f}, Unique Customers: {row[2]}\")\n",
    "\n",
    "# Query 3: Customer spend by category\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    c.city,\n",
    "    c.state,\n",
    "    p.category,\n",
    "    COUNT(DISTINCT s.customer_id) AS customer_count,\n",
    "    SUM(s.total_amount) AS total_spent\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.customers c ON s.customer_id = c.customer_id\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY c.city, c.state, p.category\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 15;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display results\n",
    "results3 = execute_sql(query3, fetch=True)\n",
    "print(\"\\nTop customer segments by spend:\")\n",
    "for row in results3:\n",
    "    print(f\"  Location: {row[0]}, {row[1]}, Category: {row[2]}, Customers: {row[3]}, Total Spent: ${row[4]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b34eab-4791-476e-83f8-e34554ea7d1d",
   "metadata": {},
   "source": [
    "### 4.3 Compare Query Performance With and Without Materialized View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d8a5bb2-9ada-4bb0-8b07-7625be2274de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query performance...\n",
      "Direct query duration: 9.96 seconds\n",
      "Materialized view query duration: 8.19 seconds\n",
      "Performance improvement with materialized view: 17.72%\n"
     ]
    }
   ],
   "source": [
    "# Compare performance between direct query and materialized view\n",
    "import time\n",
    "\n",
    "# Direct query (without materialized view)\n",
    "direct_query = \"\"\"\n",
    "SELECT \n",
    "    s.sale_date,\n",
    "    p.category,\n",
    "    COUNT(DISTINCT s.customer_id) AS unique_customers,\n",
    "    SUM(s.quantity) AS units_sold,\n",
    "    SUM(s.total_amount) AS total_revenue,\n",
    "    SUM(s.total_amount - (s.quantity * p.cost)) AS gross_profit\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY s.sale_date, p.category\n",
    "ORDER BY s.sale_date, total_revenue DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "# Materialized view query\n",
    "mv_query = \"\"\"\n",
    "SELECT *\n",
    "FROM analytics.daily_sales_by_category\n",
    "ORDER BY sale_date, total_revenue DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing query performance...\")\n",
    "\n",
    "# Test direct query\n",
    "start_time = time.time()\n",
    "execute_sql(direct_query, fetch=True)\n",
    "direct_duration = time.time() - start_time\n",
    "print(f\"Direct query duration: {direct_duration:.2f} seconds\")\n",
    "\n",
    "# Test materialized view query\n",
    "start_time = time.time()\n",
    "execute_sql(mv_query, fetch=True)\n",
    "mv_duration = time.time() - start_time\n",
    "print(f\"Materialized view query duration: {mv_duration:.2f} seconds\")\n",
    "\n",
    "# Calculate improvement\n",
    "if direct_duration > 0 and mv_duration > 0:\n",
    "    improvement = ((direct_duration - mv_duration) / direct_duration) * 100\n",
    "    print(f\"Performance improvement with materialized view: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb7595-cc09-40ec-b7c8-401c08033dcf",
   "metadata": {},
   "source": [
    "## 5. Examine Execution Plans\n",
    "\n",
    "### 5.1 Analyze Query Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97f65c7b-5a42-4350-853b-5404ed7ea41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Execution Plan:\n",
      "XN Merge  (cost=1000000000113.02..1000000000113.52 rows=200 width=230)\n",
      "  Merge Key: grvar_1, grvar_2, grvar_3\n",
      "  ->  XN Network  (cost=1000000000113.02..1000000000113.52 rows=200 width=230)\n",
      "        Send to leader\n",
      "        ->  XN Sort  (cost=1000000000113.02..1000000000113.52 rows=200 width=230)\n",
      "              Sort Key: grvar_1, grvar_2, grvar_3\n",
      "              ->  XN HashAggregate  (cost=104.38..105.38 rows=200 width=230)\n",
      "                    ->  XN Subquery Scan subq  (cost=74.38..91.88 rows=1000 width=230)\n",
      "                          ->  XN HashAggregate  (cost=74.38..81.88 rows=1000 width=40)\n",
      "                                ->  XN Hash Join DS_DIST_ALL_NONE  (cost=1.88..61.88 rows=1000 width=40)\n",
      "                                      Hash Cond: (\"outer\".customer_id = \"inner\".customer_id)\n",
      "                                      ->  XN Hash Join DS_DIST_ALL_NONE  (cost=0.62..33.12 rows=1000 width=34)\n",
      "                                            Hash Cond: (\"outer\".product_id = \"inner\".product_id)\n",
      "                                            ->  XN Seq Scan on sales s_1  (cost=0.00..10.00 rows=1000 width=28)\n",
      "                                            ->  XN Hash  (cost=0.50..0.50 rows=50 width=14)\n",
      "                                                  ->  XN Seq Scan on products p_1  (cost=0.00..0.50 rows=50 width=14)\n",
      "                                      ->  XN Hash  (cost=1.00..1.00 rows=100 width=10)\n",
      "                                            ->  XN Seq Scan on customers c_1  (cost=0.00..1.00 rows=100 width=10)\n"
     ]
    }
   ],
   "source": [
    "# Get execution plan for an analytical query\n",
    "explain_query = \"\"\"\n",
    "EXPLAIN\n",
    "SELECT \n",
    "    c.state,\n",
    "    p.category,\n",
    "    DATE_TRUNC('month', s.sale_date) AS month,\n",
    "    COUNT(DISTINCT s.customer_id) AS unique_customers,\n",
    "    SUM(s.total_amount) AS total_revenue\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.customers c ON s.customer_id = c.customer_id\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY c.state, p.category, DATE_TRUNC('month', s.sale_date)\n",
    "ORDER BY c.state, p.category, month;\n",
    "\"\"\"\n",
    "\n",
    "# Execute and display the execution plan\n",
    "execution_plan = execute_sql(explain_query, fetch=True)\n",
    "print(\"Query Execution Plan:\")\n",
    "for row in execution_plan:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959f26d-d6f7-42c0-af6b-247f9789d606",
   "metadata": {},
   "source": [
    "### 5.2 Examine Table Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2484e05-e09d-4056-9caf-59af5bd3b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Table Information:\n",
      "Table: analytics.customers\n",
      "  Distribution Style: ALL\n",
      "  Sort Key: customer_id\n",
      "  Compression Encoded: Y, AUTO(ENCODE)\n",
      "  Size: 0.00 MB\n",
      "\n",
      "Table: analytics.daily_sales_by_category\n",
      "  Distribution Style: EVEN\n",
      "  Sort Key: None\n",
      "  Compression Encoded: Y\n",
      "  Size: 0.00 MB\n",
      "\n",
      "Table: analytics.products\n",
      "  Distribution Style: ALL\n",
      "  Sort Key: product_id\n",
      "  Compression Encoded: Y, AUTO(ENCODE)\n",
      "  Size: 0.00 MB\n",
      "\n",
      "Table: analytics.sales\n",
      "  Distribution Style: KEY(customer_id)\n",
      "  Sort Key: sale_date\n",
      "  Compression Encoded: Y, AUTO(ENCODE)\n",
      "  Size: 0.00 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_info_query = \"\"\"\n",
    "SELECT \n",
    "    \"schema\" as table_schema,\n",
    "    \"table\" as table_name,\n",
    "    diststyle,\n",
    "    sortkey1,\n",
    "    encoded,\n",
    "    size/1024/1024 as size_mb\n",
    "FROM svv_table_info\n",
    "WHERE \"schema\" = 'analytics'\n",
    "ORDER BY \"schema\", \"table\";\n",
    "\"\"\"\n",
    "\n",
    "# Execute and display table information\n",
    "table_info = execute_sql(table_info_query, fetch=True)\n",
    "print(\"Redshift Table Information:\")\n",
    "for row in table_info:\n",
    "    print(f\"Table: {row[0]}.{row[1]}\")\n",
    "    print(f\"  Distribution Style: {row[2]}\")\n",
    "    print(f\"  Sort Key: {row[3]}\")\n",
    "    print(f\"  Compression Encoded: {row[4]}\")\n",
    "    print(f\"  Size: {row[5]:.2f} MB\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120355ac-2d64-4748-8ea5-0dafb12983e2",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Clean Up Resources\n",
    "\n",
    "### 6.1 Delete Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "612e0efb-5d9e-4fb1-9f12-d2f216aebfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Redshift cluster: redshift-workshop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_redshift_cluster():\n",
    "    \"\"\"Delete the Redshift cluster\"\"\"\n",
    "    redshift = boto3.client('redshift', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        # Delete the cluster without final snapshot\n",
    "        redshift.delete_cluster(\n",
    "            ClusterIdentifier=cluster_identifier,\n",
    "            SkipFinalClusterSnapshot=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Deleting Redshift cluster: {cluster_identifier}\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error deleting Redshift cluster: {e}\")\n",
    "        return False\n",
    "\n",
    "# Delete the Redshift cluster\n",
    "delete_redshift_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b2293-4083-4a5f-8ef0-9b5a7a7ec0ab",
   "metadata": {},
   "source": [
    "### 6.2 Wait for Cluster Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425dbd9a-3962-46e6-af7c-14d93c998901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for cluster redshift-workshop to be deleted...\n",
      "Cluster status: deleting. Waiting...\n",
      "Cluster status: deleting. Waiting...\n"
     ]
    }
   ],
   "source": [
    "def wait_for_cluster_deletion():\n",
    "    \"\"\"Wait for the Redshift cluster to be deleted\"\"\"\n",
    "    redshift = boto3.client('redshift', region_name=region)\n",
    "    \n",
    "    print(f\"Waiting for cluster {cluster_identifier} to be deleted...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = redshift.describe_clusters(ClusterIdentifier=cluster_identifier)\n",
    "            status = response['Clusters'][0]['ClusterStatus']\n",
    "            \n",
    "            print(f\"Cluster status: {status}. Waiting...\")\n",
    "            time.sleep(30)  # Check every 30 seconds\n",
    "            \n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ClusterNotFound':\n",
    "                print(f\"Cluster {cluster_identifier} has been deleted\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Error checking cluster status: {e}\")\n",
    "                return False\n",
    "\n",
    "# Wait for cluster deletion\n",
    "wait_for_cluster_deletion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d10ad-f63c-4da6-8ca9-c45e073b8887",
   "metadata": {},
   "source": [
    "### 6.3 Delete S3 Bucket and IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457fe7d-fb5e-4a48-ab30-5671e6989327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_resources():\n",
    "    \"\"\"Delete S3 bucket and IAM role\"\"\"\n",
    "    # Delete S3 bucket contents and bucket\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(s3_bucket)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Deleting all objects from bucket {s3_bucket}\")\n",
    "        bucket.objects.all().delete()\n",
    "        \n",
    "        print(f\"Deleting bucket {s3_bucket}\")\n",
    "        bucket.delete()\n",
    "    except ClientError as e:\n",
    "        print(f\"Error deleting S3 bucket: {e}\")\n",
    "    \n",
    "    # Detach policy and delete IAM role\n",
    "    iam = boto3.client('iam')\n",
    "    \n",
    "    try:\n",
    "        print(\"Detaching policy from IAM role\")\n",
    "        iam.detach_role_policy(\n",
    "            RoleName='RedshiftWorkshopRole',\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    "        )\n",
    "        \n",
    "        print(\"Deleting IAM role\")\n",
    "        iam.delete_role(RoleName='RedshiftWorkshopRole')\n",
    "    except ClientError as e:\n",
    "        print(f\"Error cleaning up IAM role: {e}\")\n",
    "    \n",
    "    print(\"Resource cleanup completed\")\n",
    "\n",
    "# Clean up resources\n",
    "clean_up_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d634638-b2be-4493-af85-dffb694fe7bf",
   "metadata": {},
   "source": [
    "\n",
    "## Workshop Summary\n",
    "\n",
    "In this hands-on workshop, you've successfully:\n",
    "\n",
    "1. Created an IAM role for Redshift to access S3\n",
    "2. Set up an S3 bucket with sample e-commerce data\n",
    "3. Deployed an Amazon Redshift cluster\n",
    "4. Created optimized tables with appropriate distribution and sort keys\n",
    "5. Loaded data using the COPY command\n",
    "6. Created a materialized view to improve query performance\n",
    "7. Run analytical queries and observed performance differences\n",
    "8. Examined query execution plans and table design\n",
    "9. Cleaned up all resources\n",
    "\n",
    "This workshop demonstrated the core capabilities of Amazon Redshift for data warehousing, from deployment to optimization and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ce149-57de-440d-945c-1d740e611e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
