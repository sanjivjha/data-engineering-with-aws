{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71117488-555d-414e-b82b-8024c59230fd",
   "metadata": {},
   "source": [
    "# Amazon Redshift Data Warehouse Workshop\n",
    "\n",
    "This hands-on workshop guides you through creating, configuring, and using an Amazon Redshift cluster for data warehousing. Each cell contains clear instructions that can be executed sequentially.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with appropriate permissions\n",
    "- AWS CLI configured with access credentials\n",
    "- Python 3.8+ with the following packages installed:\n",
    "  - boto3\n",
    "  - pandas\n",
    "  - psycopg2 or psycopg2-binary\n",
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34364aa3-0c55-4712-a830-465eb8174e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa8d73-3a31-4c13-9e20-002ff252c72a",
   "metadata": {},
   "source": [
    "### 1.2 Set Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c4148-146d-4aae-8397-573475282c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "region = 'us-east-1'  # Change to your preferred region\n",
    "\n",
    "# Redshift Cluster Configuration\n",
    "cluster_identifier = 'redshift-workshop'\n",
    "node_type = 'dc2.large'  # For workshop, using smaller node type\n",
    "number_of_nodes = 2\n",
    "db_name = 'dwh'\n",
    "master_username = 'admin'\n",
    "master_password = 'Redshift123!'  # Use a secure password in production\n",
    "\n",
    "# VPC Configuration - will use default VPC for simplicity\n",
    "vpc_id = 'default'  # Will be populated later\n",
    "\n",
    "# S3 Configuration\n",
    "s3_bucket = f'redshift-workshop-{int(time.time())}'  # Unique bucket name\n",
    "sample_data_prefix = 'sample-data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953718c7-2163-4486-bf61-9d35541e3647",
   "metadata": {},
   "source": [
    "## 2. Create Resources\n",
    "\n",
    "### 2.1 Create IAM Role for Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a9ad4-18f6-438a-b004-8d34d900f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_redshift_service_role():\n",
    "    \"\"\"Create IAM role for Redshift to access S3\"\"\"\n",
    "    iam = boto3.client('iam', region_name=region)\n",
    "    \n",
    "    # Define trust policy\n",
    "    trust_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"redshift.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create the role\n",
    "        response = iam.create_role(\n",
    "            RoleName='RedshiftWorkshopRole',\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "            Description='Role for Redshift workshop to access S3'\n",
    "        )\n",
    "        \n",
    "        role_arn = response['Role']['Arn']\n",
    "        print(f\"Created IAM role: RedshiftWorkshopRole with ARN: {role_arn}\")\n",
    "        \n",
    "        # Attach S3 read policy\n",
    "        iam.attach_role_policy(\n",
    "            RoleName='RedshiftWorkshopRole',\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    "        )\n",
    "        \n",
    "        print(\"Attached S3 read policy to role\")\n",
    "        return role_arn\n",
    "    \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "            print(\"IAM role RedshiftWorkshopRole already exists\")\n",
    "            response = iam.get_role(RoleName='RedshiftWorkshopRole')\n",
    "            return response['Role']['Arn']\n",
    "        else:\n",
    "            print(f\"Error creating IAM role: {e}\")\n",
    "            raise\n",
    "\n",
    "# Create the IAM role\n",
    "redshift_role_arn = create_redshift_service_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e74ee-bea3-4a86-9869-0524870c509c",
   "metadata": {},
   "source": [
    "### 2.2 Create S3 Bucket and Upload Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf05fe-0668-4dcb-9a13-5f08e1c62405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_bucket_and_sample_data():\n",
    "    \"\"\"Create S3 bucket and generate sample data\"\"\"\n",
    "    s3 = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region == 'us-east-1':\n",
    "            s3.create_bucket(Bucket=s3_bucket)\n",
    "        else:\n",
    "            s3.create_bucket(\n",
    "                Bucket=s3_bucket,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"Created S3 bucket: {s3_bucket}\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error creating bucket: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Generate sample customers data\n",
    "    customers_data = pd.DataFrame({\n",
    "        'customer_id': range(1, 101),\n",
    "        'customer_name': [f'Customer {i}' for i in range(1, 101)],\n",
    "        'email': [f'customer{i}@example.com' for i in range(1, 101)],\n",
    "        'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'] * 20,\n",
    "        'state': ['NY', 'CA', 'IL', 'TX', 'AZ'] * 20,\n",
    "        'country': ['USA'] * 100,\n",
    "        'registration_date': pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "    })\n",
    "    \n",
    "    # Generate sample products data\n",
    "    products_data = pd.DataFrame({\n",
    "        'product_id': range(1, 51),\n",
    "        'product_name': [f'Product {i}' for i in range(1, 51)],\n",
    "        'category': ['Electronics', 'Clothing', 'Home', 'Books', 'Toys'] * 10,\n",
    "        'price': [round(i * 9.99, 2) for i in range(1, 51)],\n",
    "        'cost': [round(i * 5.99, 2) for i in range(1, 51)]\n",
    "    })\n",
    "    \n",
    "    # Generate sample sales data\n",
    "    import numpy as np\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    sales_data = []\n",
    "    for i in range(1, 1001):\n",
    "        customer_id = np.random.randint(1, 101)\n",
    "        product_id = np.random.randint(1, 51)\n",
    "        product_price = products_data.loc[product_id-1, 'price']\n",
    "        quantity = np.random.randint(1, 5)\n",
    "        \n",
    "        sales_data.append({\n",
    "            'sale_id': i,\n",
    "            'customer_id': customer_id,\n",
    "            'product_id': product_id,\n",
    "            'sale_date': pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365)),\n",
    "            'quantity': quantity,\n",
    "            'unit_price': product_price,\n",
    "            'total_amount': quantity * product_price\n",
    "        })\n",
    "    \n",
    "    sales_df = pd.DataFrame(sales_data)\n",
    "    \n",
    "    # Save to local files first\n",
    "    customers_data.to_csv('customers.csv', index=False)\n",
    "    products_data.to_csv('products.csv', index=False)\n",
    "    sales_df.to_csv('sales.csv', index=False)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(s3_bucket, f\"{sample_data_prefix}customers.csv\").upload_file('customers.csv')\n",
    "    s3_resource.Object(s3_bucket, f\"{sample_data_prefix}products.csv\").upload_file('products.csv')\n",
    "    s3_resource.Object(s3_bucket, f\"{sample_data_prefix}sales.csv\").upload_file('sales.csv')\n",
    "    \n",
    "    print(f\"Uploaded sample data to s3://{s3_bucket}/{sample_data_prefix}\")\n",
    "    \n",
    "    # Clean up local files\n",
    "    os.remove('customers.csv')\n",
    "    os.remove('products.csv')\n",
    "    os.remove('sales.csv')\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Create bucket and upload sample data\n",
    "create_s3_bucket_and_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acc516-dde5-47a9-b200-43adb90ef09d",
   "metadata": {},
   "source": [
    "### 2.3 Create Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f45c8d-bc29-4a74-84d3-176f336a86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_redshift_cluster():\n",
    "    \"\"\"Create a Redshift cluster\"\"\"\n",
    "    redshift = boto3.client('redshift', region_name=region)\n",
    "    \n",
    "    # Get default VPC ID\n",
    "    ec2 = boto3.client('ec2', region_name=region)\n",
    "    vpcs = ec2.describe_vpcs(\n",
    "        Filters=[{'Name': 'isDefault', 'Values': ['true']}]\n",
    "    )\n",
    "    \n",
    "    if not vpcs['Vpcs']:\n",
    "        raise Exception(\"No default VPC found in this account/region\")\n",
    "    \n",
    "    global vpc_id\n",
    "    vpc_id = vpcs['Vpcs'][0]['VpcId']\n",
    "    print(f\"Using default VPC: {vpc_id}\")\n",
    "    \n",
    "    # Create security group for Redshift\n",
    "    try:\n",
    "        sg_response = ec2.create_security_group(\n",
    "            GroupName='redshift-workshop-sg',\n",
    "            Description='Security group for Redshift workshop',\n",
    "            VpcId=vpc_id\n",
    "        )\n",
    "        security_group_id = sg_response['GroupId']\n",
    "        \n",
    "        # Add ingress rule to allow connections\n",
    "        ec2.authorize_security_group_ingress(\n",
    "            GroupId=security_group_id,\n",
    "            IpPermissions=[\n",
    "                {\n",
    "                    'IpProtocol': 'tcp',\n",
    "                    'FromPort': 5439,\n",
    "                    'ToPort': 5439,\n",
    "                    'IpRanges': [{'CidrIp': '0.0.0.0/0'}]  # In production, restrict this!\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"Created security group: {security_group_id}\")\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'InvalidGroup.Duplicate':\n",
    "            # Get existing security group\n",
    "            sgs = ec2.describe_security_groups(\n",
    "                Filters=[\n",
    "                    {'Name': 'group-name', 'Values': ['redshift-workshop-sg']},\n",
    "                    {'Name': 'vpc-id', 'Values': [vpc_id]}\n",
    "                ]\n",
    "            )\n",
    "            security_group_id = sgs['SecurityGroups'][0]['GroupId']\n",
    "            print(f\"Using existing security group: {security_group_id}\")\n",
    "        else:\n",
    "            print(f\"Error creating security group: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Create Redshift cluster\n",
    "    try:\n",
    "        response = redshift.create_cluster(\n",
    "            ClusterIdentifier=cluster_identifier,\n",
    "            NodeType=node_type,\n",
    "            NumberOfNodes=number_of_nodes,\n",
    "            DBName=db_name,\n",
    "            MasterUsername=master_username,\n",
    "            MasterUserPassword=master_password,\n",
    "            VpcSecurityGroupIds=[security_group_id],\n",
    "            PubliclyAccessible=True,  # For workshop purposes\n",
    "            IamRoles=[redshift_role_arn]\n",
    "        )\n",
    "        \n",
    "        print(f\"Creating Redshift cluster: {cluster_identifier}\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ClusterAlreadyExists':\n",
    "            print(f\"Redshift cluster {cluster_identifier} already exists\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error creating Redshift cluster: {e}\")\n",
    "            raise\n",
    "\n",
    "# Create the Redshift cluster\n",
    "create_redshift_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2744648-f39c-4563-838b-0690c50d2c2f",
   "metadata": {},
   "source": [
    "### 2.4 Wait for Cluster to be Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b686cc-bd28-42ad-a16d-6a440a3a8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_cluster_available():\n",
    "    \"\"\"Wait for the Redshift cluster to be in 'available' state\"\"\"\n",
    "    redshift = boto3.client('redshift', region_name=region)\n",
    "    \n",
    "    print(f\"Waiting for cluster {cluster_identifier} to be available...\")\n",
    "    \n",
    "    while True:\n",
    "        response = redshift.describe_clusters(ClusterIdentifier=cluster_identifier)\n",
    "        status = response['Clusters'][0]['ClusterStatus']\n",
    "        \n",
    "        if status == 'available':\n",
    "            print(f\"Cluster {cluster_identifier} is now available\")\n",
    "            endpoint = response['Clusters'][0]['Endpoint']['Address']\n",
    "            port = response['Clusters'][0]['Endpoint']['Port']\n",
    "            return endpoint, port\n",
    "        \n",
    "        print(f\"Cluster status: {status}. Waiting...\")\n",
    "        time.sleep(30)  # Check every 30 seconds for workshop purposes\n",
    "\n",
    "# Wait for cluster to be available\n",
    "cluster_endpoint, cluster_port = wait_for_cluster_available()\n",
    "print(f\"Cluster endpoint: {cluster_endpoint}\")\n",
    "print(f\"Cluster port: {cluster_port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f68ba-aa34-455f-9394-a07f60c7161e",
   "metadata": {},
   "source": [
    "## 3. Database Setup and Data Loading\n",
    "\n",
    "### 3.1 Create a Function to Execute SQL Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca951f-875f-40c5-8ee6-8acb49bbeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(sql, fetch=False):\n",
    "    \"\"\"Execute SQL commands on the Redshift cluster\"\"\"\n",
    "    conn = None\n",
    "    result = None\n",
    "    \n",
    "    try:\n",
    "        # Connect to the Redshift cluster\n",
    "        conn = psycopg2.connect(\n",
    "            host=cluster_endpoint,\n",
    "            port=cluster_port,\n",
    "            dbname=db_name,\n",
    "            user=master_username,\n",
    "            password=master_password\n",
    "        )\n",
    "        \n",
    "        # Set isolation level to autocommit\n",
    "        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "        \n",
    "        # Create a cursor\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Execute the SQL command\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        # Fetch results if requested\n",
    "        if fetch:\n",
    "            result = cur.fetchall()\n",
    "        \n",
    "        # Close cursor\n",
    "        cur.close()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        # Close connection\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2fd06-7f1e-4832-818a-aa8650a13afd",
   "metadata": {},
   "source": [
    "### 3.2 Create Schema and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc84f3a-6974-4698-93af-b5d129fe0018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema and tables\n",
    "schema_and_tables_sql = \"\"\"\n",
    "-- Create schema\n",
    "CREATE SCHEMA IF NOT EXISTS analytics;\n",
    "\n",
    "-- Set search path\n",
    "SET search_path TO analytics, public;\n",
    "\n",
    "-- Create customers dimension table with ALL distribution\n",
    "CREATE TABLE IF NOT EXISTS analytics.customers (\n",
    "    customer_id INTEGER PRIMARY KEY,\n",
    "    customer_name VARCHAR(100),\n",
    "    email VARCHAR(100),\n",
    "    city VARCHAR(50),\n",
    "    state VARCHAR(50),\n",
    "    country VARCHAR(50),\n",
    "    registration_date DATE\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "SORTKEY(customer_id);\n",
    "\n",
    "-- Create products dimension table with ALL distribution\n",
    "CREATE TABLE IF NOT EXISTS analytics.products (\n",
    "    product_id INTEGER PRIMARY KEY,\n",
    "    product_name VARCHAR(100) NOT NULL,\n",
    "    category VARCHAR(50) NOT NULL,\n",
    "    price DECIMAL(10,2) NOT NULL,\n",
    "    cost DECIMAL(10,2)\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "SORTKEY(product_id);\n",
    "\n",
    "-- Create sales fact table with KEY distribution\n",
    "CREATE TABLE IF NOT EXISTS analytics.sales (\n",
    "    sale_id INTEGER PRIMARY KEY,\n",
    "    customer_id INTEGER NOT NULL REFERENCES analytics.customers(customer_id),\n",
    "    product_id INTEGER NOT NULL REFERENCES analytics.products(product_id),\n",
    "    sale_date DATE NOT NULL,\n",
    "    quantity INTEGER NOT NULL,\n",
    "    unit_price DECIMAL(10,2) NOT NULL,\n",
    "    total_amount DECIMAL(10,2) NOT NULL\n",
    ")\n",
    "DISTKEY(customer_id)\n",
    "COMPOUND SORTKEY(sale_date, customer_id);\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(schema_and_tables_sql)\n",
    "print(\"Created schema and tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8918e8f2-a0d9-4d53-af9c-607194efc702",
   "metadata": {},
   "source": [
    "### 3.3 Load Data from S3 Using COPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684bece-592e-40cb-a832-67b881c34308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into tables using COPY command\n",
    "load_data_sql = f\"\"\"\n",
    "-- Load customers data\n",
    "COPY analytics.customers\n",
    "FROM 's3://{s3_bucket}/{sample_data_prefix}customers.csv'\n",
    "IAM_ROLE '{redshift_role_arn}'\n",
    "FORMAT AS CSV DELIMITER ',' IGNOREHEADER 1\n",
    "REGION '{region}';\n",
    "\n",
    "-- Load products data\n",
    "COPY analytics.products\n",
    "FROM 's3://{s3_bucket}/{sample_data_prefix}products.csv'\n",
    "IAM_ROLE '{redshift_role_arn}'\n",
    "FORMAT AS CSV DELIMITER ',' IGNOREHEADER 1\n",
    "REGION '{region}';\n",
    "\n",
    "-- Load sales data\n",
    "COPY analytics.sales\n",
    "FROM 's3://{s3_bucket}/{sample_data_prefix}sales.csv'\n",
    "IAM_ROLE '{redshift_role_arn}'\n",
    "FORMAT AS CSV DELIMITER ',' IGNOREHEADER 1\n",
    "REGION '{region}';\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(load_data_sql)\n",
    "print(\"Loaded data from S3 to Redshift tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ae2b07-66c0-4d3b-8ea0-887713df957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.4 Verify Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179f123-7a45-40c6-a14b-5e9bbf468f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data was loaded correctly\n",
    "verify_load_sql = \"\"\"\n",
    "SELECT 'customers' AS table_name, COUNT(*) AS row_count FROM analytics.customers\n",
    "UNION ALL\n",
    "SELECT 'products', COUNT(*) FROM analytics.products\n",
    "UNION ALL\n",
    "SELECT 'sales', COUNT(*) FROM analytics.sales;\n",
    "\"\"\"\n",
    "\n",
    "row_counts = execute_sql(verify_load_sql, fetch=True)\n",
    "print(\"Table row counts:\")\n",
    "for row in row_counts:\n",
    "    print(f\"  {row[0]}: {row[1]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755f92d-95dc-467f-b1b2-f9112de947a2",
   "metadata": {},
   "source": [
    "## 4. Running Analytical Queries\n",
    "\n",
    "### 4.1 Create a Materialized View for Common Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3a327-ebe7-4598-8fb6-de75279f62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a materialized view for sales by category\n",
    "materialized_view_sql = \"\"\"\n",
    "-- Create materialized view for daily sales by category\n",
    "CREATE MATERIALIZED VIEW analytics.daily_sales_by_category AS\n",
    "SELECT \n",
    "    s.sale_date,\n",
    "    p.category,\n",
    "    COUNT(DISTINCT s.customer_id) AS unique_customers,\n",
    "    SUM(s.quantity) AS units_sold,\n",
    "    SUM(s.total_amount) AS total_revenue,\n",
    "    SUM(s.total_amount - (s.quantity * p.cost)) AS gross_profit\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY s.sale_date, p.category;\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(materialized_view_sql)\n",
    "print(\"Created materialized view for sales by category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d70e62-6d71-4cd8-b8a3-890354367354",
   "metadata": {},
   "source": [
    "### 4.2 Run Analytical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f744dc-7e16-451e-af4d-625f96f7455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sample analytical queries\n",
    "\n",
    "# Query 1: Top selling products\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    p.product_id,\n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    SUM(s.quantity) AS total_quantity_sold,\n",
    "    SUM(s.total_amount) AS total_revenue\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY p.product_id, p.product_name, p.category\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display results\n",
    "results1 = execute_sql(query1, fetch=True)\n",
    "print(\"Top 10 selling products by revenue:\")\n",
    "for row in results1:\n",
    "    print(f\"  Product ID: {row[0]}, Name: {row[1]}, Category: {row[2]}, Quantity: {row[3]}, Revenue: ${row[4]:.2f}\")\n",
    "\n",
    "# Query 2: Monthly sales trends\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    DATE_TRUNC('month', sale_date) AS month,\n",
    "    SUM(total_amount) AS monthly_revenue,\n",
    "    COUNT(DISTINCT customer_id) AS unique_customers\n",
    "FROM analytics.sales\n",
    "GROUP BY DATE_TRUNC('month', sale_date)\n",
    "ORDER BY month;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display results\n",
    "results2 = execute_sql(query2, fetch=True)\n",
    "print(\"\\nMonthly sales trends:\")\n",
    "for row in results2:\n",
    "    print(f\"  Month: {row[0]}, Revenue: ${row[1]:.2f}, Unique Customers: {row[2]}\")\n",
    "\n",
    "# Query 3: Customer spend by category\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    c.city,\n",
    "    c.state,\n",
    "    p.category,\n",
    "    COUNT(DISTINCT s.customer_id) AS customer_count,\n",
    "    SUM(s.total_amount) AS total_spent\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.customers c ON s.customer_id = c.customer_id\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY c.city, c.state, p.category\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 15;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display results\n",
    "results3 = execute_sql(query3, fetch=True)\n",
    "print(\"\\nTop customer segments by spend:\")\n",
    "for row in results3:\n",
    "    print(f\"  Location: {row[0]}, {row[1]}, Category: {row[2]}, Customers: {row[3]}, Total Spent: ${row[4]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b34eab-4791-476e-83f8-e34554ea7d1d",
   "metadata": {},
   "source": [
    "### 4.3 Compare Query Performance With and Without Materialized View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a5bb2-9ada-4bb0-8b07-7625be2274de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance between direct query and materialized view\n",
    "import time\n",
    "\n",
    "# Direct query (without materialized view)\n",
    "direct_query = \"\"\"\n",
    "SELECT \n",
    "    s.sale_date,\n",
    "    p.category,\n",
    "    COUNT(DISTINCT s.customer_id) AS unique_customers,\n",
    "    SUM(s.quantity) AS units_sold,\n",
    "    SUM(s.total_amount) AS total_revenue,\n",
    "    SUM(s.total_amount - (s.quantity * p.cost)) AS gross_profit\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY s.sale_date, p.category\n",
    "ORDER BY s.sale_date, total_revenue DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "# Materialized view query\n",
    "mv_query = \"\"\"\n",
    "SELECT *\n",
    "FROM analytics.daily_sales_by_category\n",
    "ORDER BY sale_date, total_revenue DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing query performance...\")\n",
    "\n",
    "# Test direct query\n",
    "start_time = time.time()\n",
    "execute_sql(direct_query, fetch=True)\n",
    "direct_duration = time.time() - start_time\n",
    "print(f\"Direct query duration: {direct_duration:.2f} seconds\")\n",
    "\n",
    "# Test materialized view query\n",
    "start_time = time.time()\n",
    "execute_sql(mv_query, fetch=True)\n",
    "mv_duration = time.time() - start_time\n",
    "print(f\"Materialized view query duration: {mv_duration:.2f} seconds\")\n",
    "\n",
    "# Calculate improvement\n",
    "if direct_duration > 0 and mv_duration > 0:\n",
    "    improvement = ((direct_duration - mv_duration) / direct_duration) * 100\n",
    "    print(f\"Performance improvement with materialized view: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb7595-cc09-40ec-b7c8-401c08033dcf",
   "metadata": {},
   "source": [
    "## 5. Examine Execution Plans\n",
    "\n",
    "### 5.1 Analyze Query Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f65c7b-5a42-4350-853b-5404ed7ea41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get execution plan for an analytical query\n",
    "explain_query = \"\"\"\n",
    "EXPLAIN\n",
    "SELECT \n",
    "    c.state,\n",
    "    p.category,\n",
    "    DATE_TRUNC('month', s.sale_date) AS month,\n",
    "    COUNT(DISTINCT s.customer_id) AS unique_customers,\n",
    "    SUM(s.total_amount) AS total_revenue\n",
    "FROM analytics.sales s\n",
    "JOIN analytics.customers c ON s.customer_id = c.customer_id\n",
    "JOIN analytics.products p ON s.product_id = p.product_id\n",
    "GROUP BY c.state, p.category, DATE_TRUNC('month', s.sale_date)\n",
    "ORDER BY c.state, p.category, month;\n",
    "\"\"\"\n",
    "\n",
    "# Execute and display the execution plan\n",
    "execution_plan = execute_sql(explain_query, fetch=True)\n",
    "print(\"Query Execution Plan:\")\n",
    "for row in execution_plan:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959f26d-d6f7-42c0-af6b-247f9789d606",
   "metadata": {},
   "source": [
    "### 5.2 Examine Table Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2484e05-e09d-4056-9caf-59af5bd3b2a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'execute_sql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m table_info_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m as table_schema,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mORDER BY \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Execute and display table information\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m table_info \u001b[38;5;241m=\u001b[39m execute_sql(table_info_query, fetch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRedshift Table Information:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table_info:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'execute_sql' is not defined"
     ]
    }
   ],
   "source": [
    "table_info_query = \"\"\"\n",
    "SELECT \n",
    "    \"schema\" as table_schema,\n",
    "    \"table\" as table_name,\n",
    "    diststyle,\n",
    "    sortkey1,\n",
    "    encoded,\n",
    "    size/1024/1024 as size_mb\n",
    "FROM svv_table_info\n",
    "WHERE \"schema\" = 'analytics'\n",
    "ORDER BY \"schema\", \"table\";\n",
    "\"\"\"\n",
    "\n",
    "# Execute and display table information\n",
    "table_info = execute_sql(table_info_query, fetch=True)\n",
    "print(\"Redshift Table Information:\")\n",
    "for row in table_info:\n",
    "    print(f\"Table: {row[0]}.{row[1]}\")\n",
    "    print(f\"  Distribution Style: {row[2]}\")\n",
    "    print(f\"  Sort Key: {row[3]}\")\n",
    "    print(f\"  Compression Encoded: {row[4]}\")\n",
    "    print(f\"  Size: {row[5]:.2f} MB\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120355ac-2d64-4748-8ea5-0dafb12983e2",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Clean Up Resources\n",
    "\n",
    "### 6.1 Delete Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612e0efb-5d9e-4fb1-9f12-d2f216aebfb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boto3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Delete the Redshift cluster\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m delete_redshift_cluster()\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36mdelete_redshift_cluster\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelete_redshift_cluster\u001b[39m():\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Delete the Redshift cluster\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     redshift \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredshift\u001b[39m\u001b[38;5;124m'\u001b[39m, region_name\u001b[38;5;241m=\u001b[39mregion)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# Delete the cluster without final snapshot\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         redshift\u001b[38;5;241m.\u001b[39mdelete_cluster(\n\u001b[0;32m      8\u001b[0m             ClusterIdentifier\u001b[38;5;241m=\u001b[39mcluster_identifier,\n\u001b[0;32m      9\u001b[0m             SkipFinalClusterSnapshot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boto3' is not defined"
     ]
    }
   ],
   "source": [
    "def delete_redshift_cluster():\n",
    "    \"\"\"Delete the Redshift cluster\"\"\"\n",
    "    redshift = boto3.client('redshift', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        # Delete the cluster without final snapshot\n",
    "        redshift.delete_cluster(\n",
    "            ClusterIdentifier=cluster_identifier,\n",
    "            SkipFinalClusterSnapshot=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Deleting Redshift cluster: {cluster_identifier}\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error deleting Redshift cluster: {e}\")\n",
    "        return False\n",
    "\n",
    "# Delete the Redshift cluster\n",
    "delete_redshift_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b2293-4083-4a5f-8ef0-9b5a7a7ec0ab",
   "metadata": {},
   "source": [
    "### 6.2 Wait for Cluster Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425dbd9a-3962-46e6-af7c-14d93c998901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_cluster_deletion():\n",
    "    \"\"\"Wait for the Redshift cluster to be deleted\"\"\"\n",
    "    redshift = boto3.client('redshift', region_name=region)\n",
    "    \n",
    "    print(f\"Waiting for cluster {cluster_identifier} to be deleted...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = redshift.describe_clusters(ClusterIdentifier=cluster_identifier)\n",
    "            status = response['Clusters'][0]['ClusterStatus']\n",
    "            \n",
    "            print(f\"Cluster status: {status}. Waiting...\")\n",
    "            time.sleep(30)  # Check every 30 seconds\n",
    "            \n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ClusterNotFound':\n",
    "                print(f\"Cluster {cluster_identifier} has been deleted\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Error checking cluster status: {e}\")\n",
    "                return False\n",
    "\n",
    "# Wait for cluster deletion\n",
    "wait_for_cluster_deletion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d10ad-f63c-4da6-8ca9-c45e073b8887",
   "metadata": {},
   "source": [
    "### 6.3 Delete S3 Bucket and IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457fe7d-fb5e-4a48-ab30-5671e6989327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_resources():\n",
    "    \"\"\"Delete S3 bucket and IAM role\"\"\"\n",
    "    # Delete S3 bucket contents and bucket\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(s3_bucket)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Deleting all objects from bucket {s3_bucket}\")\n",
    "        bucket.objects.all().delete()\n",
    "        \n",
    "        print(f\"Deleting bucket {s3_bucket}\")\n",
    "        bucket.delete()\n",
    "    except ClientError as e:\n",
    "        print(f\"Error deleting S3 bucket: {e}\")\n",
    "    \n",
    "    # Detach policy and delete IAM role\n",
    "    iam = boto3.client('iam')\n",
    "    \n",
    "    try:\n",
    "        print(\"Detaching policy from IAM role\")\n",
    "        iam.detach_role_policy(\n",
    "            RoleName='RedshiftWorkshopRole',\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    "        )\n",
    "        \n",
    "        print(\"Deleting IAM role\")\n",
    "        iam.delete_role(RoleName='RedshiftWorkshopRole')\n",
    "    except ClientError as e:\n",
    "        print(f\"Error cleaning up IAM role: {e}\")\n",
    "    \n",
    "    print(\"Resource cleanup completed\")\n",
    "\n",
    "# Clean up resources\n",
    "clean_up_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d634638-b2be-4493-af85-dffb694fe7bf",
   "metadata": {},
   "source": [
    "\n",
    "## Workshop Summary\n",
    "\n",
    "In this hands-on workshop, you've successfully:\n",
    "\n",
    "1. Created an IAM role for Redshift to access S3\n",
    "2. Set up an S3 bucket with sample e-commerce data\n",
    "3. Deployed an Amazon Redshift cluster\n",
    "4. Created optimized tables with appropriate distribution and sort keys\n",
    "5. Loaded data using the COPY command\n",
    "6. Created a materialized view to improve query performance\n",
    "7. Run analytical queries and observed performance differences\n",
    "8. Examined query execution plans and table design\n",
    "9. Cleaned up all resources\n",
    "\n",
    "This workshop demonstrated the core capabilities of Amazon Redshift for data warehousing, from deployment to optimization and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ce149-57de-440d-945c-1d740e611e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
