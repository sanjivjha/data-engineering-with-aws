{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Data Format Migration Demo\n",
    "\n",
    "This notebook demonstrates the practical implementation of data format migration and optimization techniques discussed in Chapter 3 of \"Modern Data Engineering on AWS\".\n",
    "\n",
    "## Setup\n",
    "First, let's import required libraries and initialize our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils import (\n",
    "    AWSManager, \n",
    "    DataGenerator, \n",
    "    PerformanceAnalyzer,\n",
    "    load_config,\n",
    "    setup_spark_session\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Environment\n",
    "Load configuration and set up AWS resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "# Initialize AWS Manager\n",
    "aws_manager = AWSManager()\n",
    "\n",
    "# Create required buckets\n",
    "aws_manager.create_buckets(config)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = setup_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Data\n",
    "Create realistic retail data for our migration demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize data generator\n",
    "data_generator = DataGenerator(config)\n",
    "\n",
    "# Generate transaction data\n",
    "transactions_df = data_generator.generate_transactions()\n",
    "print(\"\\nTransaction Data Sample:\")\n",
    "display(transactions_df.head())\n",
    "\n",
    "# Generate customer data\n",
    "customers_df = data_generator.generate_customers()\n",
    "print(\"\\nCustomer Data Sample:\")\n",
    "display(customers_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Storage\n",
    "Store data in original formats (CSV and JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Store transaction data as CSV\n",
    "transactions_path = f\"s3://{config['aws']['source_bucket']}/transactions/data.csv\"\n",
    "transactions_df.to_csv(transactions_path, index=False)\n",
    "\n",
    "# Store customer data as JSON\n",
    "customers_path = f\"s3://{config['aws']['source_bucket']}/customers/data.json\"\n",
    "customers_df.to_json(customers_path, orient='records', lines=True)\n",
    "\n",
    "print(\"Data stored in original formats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Initial Storage Metrics\n",
    "Measure storage usage and access patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize performance analyzer\n",
    "performance_analyzer = PerformanceAnalyzer(spark)\n",
    "\n",
    "# Analyze initial storage metrics\n",
    "initial_metrics = {\n",
    "    'transactions': performance_analyzer.analyze_storage_metrics(\n",
    "        config['aws']['source_bucket'], 'transactions/'\n",
    "    ),\n",
    "    'customers': performance_analyzer.analyze_storage_metrics(\n",
    "        config['aws']['source_bucket'], 'customers/'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Display initial metrics\n",
    "pd.DataFrame(initial_metrics).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform Format Migration\n",
    "Migrate data to optimized formats (Parquet and Delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert transactions to Delta format\n",
    "spark_df = spark.createDataFrame(transactions_df)\n",
    "delta_path = f\"s3://{config['aws']['target_bucket']}/transactions_delta\"\n",
    "\n",
    "# Write as Delta format with partitioning\n",
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "# Convert customers to Parquet format\n",
    "parquet_path = f\"s3://{config['aws']['target_bucket']}/customers_parquet\"\n",
    "customers_df.to_parquet(\n",
    "    parquet_path,\n",
    "    partition_cols=['country'],\n",
    "    compression='snappy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Performance\n",
    "Measure and compare performance between original and optimized formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Measure storage after migration\n",
    "optimized_metrics = {\n",
    "    'transactions_delta': performance_analyzer.analyze_storage_metrics(\n",
    "        config['aws']['target_bucket'], 'transactions_delta/'\n",
    "    ),\n",
    "    'customers_parquet': performance_analyzer.analyze_storage_metrics(\n",
    "        config['aws']['target_bucket'], 'customers_parquet/'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Compare query performance\n",
    "test_query = \"\"\"\n",
    "    SELECT category, \n",
    "           COUNT(*) as transaction_count, \n",
    "           SUM(amount) as total_amount\n",
    "    FROM delta.`{}` \n",
    "    GROUP BY category\n",
    "\"\"\".format(delta_path)\n",
    "\n",
    "query_metrics = performance_analyzer.measure_query_performance(test_query)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nStorage Comparison:\")\n",
    "display(pd.DataFrame({\n",
    "    'Original': initial_metrics,\n",
    "    'Optimized': optimized_metrics\n",
    "}))\n",
    "\n",
    "print(\"\\nQuery Performance:\")\n",
    "display(pd.DataFrame(query_metrics, index=[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comparison visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Storage comparison\n",
    "storage_data = pd.DataFrame({\n",
    "    'Original': [initial_metrics['transactions']['total_size_gb'], \n",
    "                initial_metrics['customers']['total_size_gb']],\n",
    "    'Optimized': [optimized_metrics['transactions_delta']['total_size_gb'],\n",
    "                  optimized_metrics['customers_parquet']['total_size_gb']]\n",
    "}, index=['Transactions', 'Customers'])\n",
    "\n",
    "storage_data.plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Storage Size Comparison (GB)')\n",
    "ax1.set_ylabel('Size (GB)')\n",
    "\n",
    "# Add percentage improvements\n",
    "for i in range(len(storage_data)):\n",
    "    pct_change = ((storage_data['Original'][i] - storage_data['Optimized'][i]) \n",
    "                  / storage_data['Original'][i] * 100)\n",
    "    ax1.text(i, storage_data['Optimized'][i], \n",
    "             f'{pct_change:.1f}% reduction', \n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean Up Resources\n",
    "Remove created AWS resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clean up buckets\n",
    "aws_manager.clean_up_buckets(config)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}